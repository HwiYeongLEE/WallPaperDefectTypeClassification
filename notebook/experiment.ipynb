{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "from PIL import Image\n",
    "from util import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from transformers import SwinForImageClassification, AutoImageProcessor, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "import wandb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f949930f",
   "metadata": {},
   "source": [
    "## Define Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc497f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=WallPaperDefectTypeClassification\n",
      "env: WANDB_NOTEBOOK_NAME=./experiment.ipynb\n",
      "env: WANDB_LOG_MODEL=end\n",
      "env: WANDB_WATCH=all\n",
      "env: WANDB_RUN_GROUP=exp3\n",
      "env: WANDB_JOB_TYPE=train\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=WallPaperDefectTypeClassification\n",
    "%env WANDB_NOTEBOOK_NAME=./experiment.ipynb\n",
    "%env WANDB_LOG_MODEL=end\n",
    "%env WANDB_WATCH=all\n",
    "%env WANDB_RUN_GROUP=exp3\n",
    "%env WANDB_JOB_TYPE=train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13862e3-bb27-47af-9b58-a9fbf804df71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device.type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7df3f2-62d0-4499-a46e-47d01699def0",
   "metadata": {},
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3367399-9798-4e38-967b-fd2320b9a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':224,\n",
    "    'EPOCHS':12,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    'BATCH_SIZE':8,\n",
    "    'WEIGHT_DECAY':0.01,\n",
    "    'WARMUP_RATIO':0.1,\n",
    "    'SEED':17,\n",
    "    'NUM_WORKERS':4,\n",
    "    'PRETRAINED_MODEL': \"microsoft/swin-base-patch4-window7-224-in22k\",\n",
    "    'MODEL_VER' : \"0.0.4_base\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254e860-ff82-43ba-bfa3-fcee4eb3ddbd",
   "metadata": {},
   "source": [
    "## Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "101a714b-71b6-4475-a4ce-fa5f98bc2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4172e-5791-446f-9616-35c09d8bf25a",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a62c78cd-4f40-4e98-b8a6-1b6f1d906b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_list = glob.glob('../data/train/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4119733d-adef-436c-afca-4112a9225d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['img_path', 'label'])\n",
    "df['img_path'] = all_img_list\n",
    "df['label'] = df['img_path'].apply(lambda x: str(x).split('/')[-2]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db41c93-3515-4fcd-936b-0a01f5388b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, _, _ = train_test_split(df, df['label'], test_size=0.3, stratify=df['label'], random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac27ed36-8031-47a7-bd0d-a913513f2e8e",
   "metadata": {},
   "source": [
    "## CustomDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89de0063",
   "metadata": {},
   "source": [
    "<img src=https://d2.naver.com/content/images/2021/01/efbe9400-5214-11eb-9c67-30fab62770ec.png>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "085f7f5e",
   "metadata": {},
   "source": [
    "**Albumentation Tutorials**<br>\n",
    "https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/migrating_from_torchvision_to_albumentations.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16fd60a5-24e2-4539-bfd0-1c374a641699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, transforms=None, processor=None):\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "        self.transforms = transforms\n",
    "        self.processor = processor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img_path = self.img_path_list[index]\n",
    "        image = Image.open(img_path)\n",
    "        image_tr = self.transforms(image=np.array(image))['image']\n",
    "        pixel_values = self.processor(image_tr, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "        \n",
    "        if self.label_list is not None:\n",
    "            label = self.label_list[index]\n",
    "            return {\n",
    "                'pixel_values': pixel_values, \n",
    "                'label': label,\n",
    "                }\n",
    "        else:\n",
    "            return {\n",
    "                'pixel_values': pixel_values,\n",
    "                }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "340b4a8b-5d6c-413f-b8b6-066e91a660e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "                            A.HorizontalFlip(p=0.5),\n",
    "                            A.RandomBrightnessContrast(p=0.5),\n",
    "                            A.RandomScale(scale_limit=0.1, p=0.5),\n",
    "                            A.RandomCropFromBorders(p=0.5),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                            # Augmentations\n",
    "                            ToTensorV2()\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "281c81d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = CFG['PRETRAINED_MODEL']\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d880481-1965-499d-9caa-fdfa8526f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(\n",
    "    train['img_path'].values, train['label'].values, train_transform, image_processor)\n",
    "\n",
    "val_dataset = CustomDataset(\n",
    "    val['img_path'].values, val['label'].values, test_transform, image_processor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39962463-032f-490a-a76d-c03991795f38",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0267ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    labels = pd.read_csv(\"../data/map.csv\")['Categories']\n",
    "    model = SwinForImageClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=len(labels),\n",
    "        id2label={str(i): c for i, c in enumerate(labels)},\n",
    "        label2id={c: str(i) for i, c in enumerate(labels)},\n",
    "        ignore_mismatched_sizes=True,\n",
    "    ).to(device)\n",
    "    print(\"Model Initialized\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122af0aa-a1fd-4595-9488-35761e3cb596",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b82cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "  return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e4f9c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch]).type(torch.LongTensor),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24435d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric' : {\n",
    "        'name': 'eval/f1',\n",
    "        'goal': 'maximize'   \n",
    "        },\n",
    "    'parameters' : {\n",
    "        'learning_rate': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 1e-5,\n",
    "            'max': 1e-3\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'distribution': 'q_uniform',\n",
    "            'min': 0,\n",
    "            'max': 1e-2,\n",
    "            'q': 0.001\n",
    "        },\n",
    "        'warmup_ratio':{\n",
    "            'values': [0.1, 0.2, 0.3]\n",
    "        },\n",
    "        'lr_scheduler_type':{\n",
    "            'values': ['linear', 'cosine']\n",
    "        },\n",
    "        'batch_size':{\n",
    "            'value': 8\n",
    "        }\n",
    "    },\n",
    "    'early_terminate': {\n",
    "        'type': 'hyperband',\n",
    "        'max_iter': CFG['EPOCHS'],\n",
    "        's': 2, \n",
    "        'eta': 2    \n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "589dd1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 0lgh0myh\n",
      "Sweep URL: https://wandb.ai/2gnldud/WallPaperDefectTypeClassification/sweeps/0lgh0myh\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=os.environ['WANDB_PROJECT'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30d94ecf",
   "metadata": {},
   "source": [
    "```\n",
    "class SchedulerType(ExplicitEnum):\n",
    "    LINEAR = \"linear\"\n",
    "    COSINE = \"cosine\"\n",
    "    COSINE_WITH_RESTARTS = \"cosine_with_restarts\"\n",
    "    POLYNOMIAL = \"polynomial\"\n",
    "    CONSTANT = \"constant\"\n",
    "    CONSTANT_WITH_WARMUP = \"constant_with_warmup\"\n",
    "    INVERSE_SQRT = \"inverse_sqrt\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9cb3a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "def train(config=None):\n",
    "    wandb.init(config=config)\n",
    "    config = wandb.config\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"../outputs/{model_name}-finetuned\",\n",
    "        overwrite_output_dir=True,\n",
    "        remove_unused_columns=False,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=config.learning_rate,\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        per_device_eval_batch_size=config.batch_size,\n",
    "        num_train_epochs=CFG['EPOCHS'],\n",
    "        weight_decay=config.weight_decay,\n",
    "        logging_steps=10,\n",
    "        gradient_accumulation_steps=4,\n",
    "        dataloader_num_workers=CFG['NUM_WORKERS'],\n",
    "        warmup_ratio=config.warmup_ratio,\n",
    "        fp16=True,\n",
    "        lr_scheduler_type=config.lr_scheduler_type,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        run_name='v.'+CFG['MODEL_VER'],\n",
    "        report_to='wandb',\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model_init=model_init,\n",
    "        args=args,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=image_processor,\n",
    "        # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    print(\"Starting Train\")\n",
    "    trainer.train()\n",
    "    wandb.finish\n",
    "    print(\"Train Ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "407e4c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y2iz5xak with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.328539964254773e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_scheduler_type: linear\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m2gnldud\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hylee/repo/WallPaperDefectTypeClassification/notebook/wandb/run-20230502_105402-y2iz5xak</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/2gnldud/WallPaperDefectTypeClassification/runs/y2iz5xak' target=\"_blank\">glorious-sweep-1</a></strong> to <a href='https://wandb.ai/2gnldud/WallPaperDefectTypeClassification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/2gnldud/WallPaperDefectTypeClassification/sweeps/0lgh0myh' target=\"_blank\">https://wandb.ai/2gnldud/WallPaperDefectTypeClassification/sweeps/0lgh0myh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/2gnldud/WallPaperDefectTypeClassification' target=\"_blank\">https://wandb.ai/2gnldud/WallPaperDefectTypeClassification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/2gnldud/WallPaperDefectTypeClassification/sweeps/0lgh0myh' target=\"_blank\">https://wandb.ai/2gnldud/WallPaperDefectTypeClassification/sweeps/0lgh0myh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/2gnldud/WallPaperDefectTypeClassification/runs/y2iz5xak' target=\"_blank\">https://wandb.ai/2gnldud/WallPaperDefectTypeClassification/runs/y2iz5xak</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224-in22k and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([19, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized\n",
      "Starting Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224-in22k and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([19, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr_scheduler_type' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='379' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [379/900 04:03 < 05:36, 1.55 it/s, Epoch 4.99/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.404100</td>\n",
       "      <td>1.116514</td>\n",
       "      <td>0.652665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.714600</td>\n",
       "      <td>0.642779</td>\n",
       "      <td>0.799573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.340700</td>\n",
       "      <td>0.533228</td>\n",
       "      <td>0.825001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.236900</td>\n",
       "      <td>0.497623</td>\n",
       "      <td>0.850250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train, count=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
